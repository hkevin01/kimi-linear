# Kimi Linear Optimization Project

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.6+-ee4c2c.svg)](https://pytorch.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**An optimized implementation of the Kimi Linear architecture - a hybrid linear attention mechanism outperforming traditional full attention.**

[Installation](#installation) â€¢ [Quick Start](#quick-start) â€¢ [Documentation](#documentation) â€¢ [Benchmarks](#benchmarks) â€¢ [Contributing](#contributing)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Architecture](#architecture)
- [Performance](#performance)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Development](#development)
- [Benchmarks](#benchmarks)
- [Citation](#citation)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## ğŸ” Overview

**Kimi Linear** is a groundbreaking hybrid attention architecture that combines the best of both worlds: the efficiency of linear attention and the performance of full attention mechanisms. This implementation focuses on optimization, hardware efficiency, and production deployment.

### What is Kimi Linear?

Kimi Linear introduces **Kimi Delta Attention (KDA)**, a linear attention mechanism with:
- **Fine-grained gating**: Channel-wise decay for precise memory control
- **Hardware-efficient algorithms**: Specialized DPLR variant optimized for modern GPUs
- **Hybrid architecture**: 3:1 KDA-to-MLA ratio for optimal performance/efficiency

### Why Kimi Linear?

- **ğŸš€ 6Ã— faster decoding** at 1M token contexts
- **ğŸ’¾ 75% KV cache reduction** for long sequences
- **ğŸ“Š Superior accuracy**: Matches or exceeds full attention on all benchmarks
- **âš¡ Linear complexity**: O(n) vs O(nÂ²) for standard attention
- **ğŸ¯ Production-ready**: vLLM integration, Docker support

### Project Purpose

This project aims to create a **production-ready, optimized implementation** of the Kimi Linear architecture for researchers and engineers working on:

1. **Long-Context Language Models**: Process sequences up to 1M tokens efficiently
2. **Agentic AI Systems**: Enable fast test-time scaling with RL training
3. **Resource-Constrained Deployment**: Reduce memory and compute requirements
4. **Research & Development**: Provide modular, well-documented codebase for experimentation

**Why This Project Exists:**
- ğŸ“š **Educational**: Clear, documented implementation of cutting-edge attention mechanisms
- ğŸ”¬ **Research**: Modular architecture for experimentation with linear attention variants
- ğŸš€ **Production**: Optimized kernels and efficient memory management for deployment
- ğŸŒ **Open Source**: Community-driven development with transparent benchmarks

---

## âœ¨ Key Features

### Core Implementation

- **Kimi Delta Attention (KDA)**
  - Fine-grained channel-wise gating mechanism
  - Hardware-efficient chunkwise parallelization
  - Delta rule learning with online gradient descent
  - Constrained DPLR formulation for numerical stability

- **Hybrid Architecture**
  - 3:1 KDA-to-MLA ratio (configurable)
  - Multi-Head Latent Attention (MLA) for global context
  - No Position Encoding (NoPE) design
  - Seamless integration with existing frameworks

### Optimization

- **CUDA/Triton Kernels**
  - Fused attention kernels
  - Memory-efficient tiling strategies
  - >80% memory bandwidth utilization
  - 2Ã— faster than general DPLR implementations

- **Memory Management**
  - Fixed-size state (constant memory)
  - Efficient buffer reuse
  - Secondary chunking for numerical stability
  - Mixed precision support (FP16, BF16, FP32)

### Testing & Validation

- **Comprehensive Test Suite**
  - Unit tests (>95% coverage target)
  - Synthetic tasks (Palindrome, MQAR, Stack)
  - Integration tests
  - Benchmark framework

- **Performance Profiling**
  - Kernel-level analysis (Nsight Compute)
  - System-level profiling (Nsight Systems)
  - Memory bandwidth monitoring
  - Automated regression testing

---

## ğŸ—ï¸ Architecture

### High-Level Architecture

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e1e1e','primaryTextColor':'#fff','primaryBorderColor':'#7c3aed','lineColor':'#f39c12','secondaryColor':'#2c3e50','tertiaryColor':'#1e1e1e','background':'#1e1e1e','mainBkg':'#2c3e50','secondBkg':'#34495e','tertiaryBkg':'#2c3e50','textColor':'#ecf0f1','fontSize':'16px'}}}%%
graph TB
    A["ğŸ”¤ Input Token Embeddings<br/>(Batch Ã— SeqLen Ã— Dim)"] --> B["âš¡ KDA Layer 1<br/>Fine-grained Gating + Delta Rule"]
    B --> C["âš¡ KDA Layer 2<br/>State Update: St âˆˆ R^(dkÃ—dv)"]
    C --> D["âš¡ KDA Layer 3<br/>Chunkwise Parallelization"]
    D --> E["ğŸŒ MLA Layer 1<br/>Global Attention (NoPE)"]
    E --> F["ğŸ“Š Feed-Forward + MoE<br/>8 of 256 Experts Activated"]
    F --> G{"More Layers?"}
    G -->|Yes| B
    G -->|No| H["ğŸ“¤ Output Logits<br/>(Batch Ã— SeqLen Ã— VocabSize)"]

    style A fill:#2c3e50,stroke:#3498db,stroke-width:3px,color:#ecf0f1
    style B fill:#2c3e50,stroke:#9b59b6,stroke-width:3px,color:#ecf0f1
    style C fill:#2c3e50,stroke:#9b59b6,stroke-width:3px,color:#ecf0f1
    style D fill:#2c3e50,stroke:#9b59b6,stroke-width:3px,color:#ecf0f1
    style E fill:#2c3e50,stroke:#e74c3c,stroke-width:3px,color:#ecf0f1
    style F fill:#2c3e50,stroke:#f39c12,stroke-width:3px,color:#ecf0f1
    style G fill:#34495e,stroke:#95a5a6,stroke-width:2px,color:#ecf0f1
    style H fill:#2c3e50,stroke:#27ae60,stroke-width:3px,color:#ecf0f1
```

### Kimi Delta Attention (KDA) Internal Flow

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e1e1e','primaryTextColor':'#fff','primaryBorderColor':'#7c3aed','lineColor':'#f39c12','secondaryColor':'#2c3e50','tertiaryColor':'#1e1e1e','background':'#1e1e1e','mainBkg':'#2c3e50','secondBkg':'#34495e','textColor':'#ecf0f1','fontSize':'14px'}}}%%
graph LR
    A["ğŸ“¥ Input x<br/>(BÃ—TÃ—D)"] --> B["ğŸ”€ Q/K/V Projection<br/>Linear + ShortConv + Swish"]
    B --> C["ğŸ“ L2Norm(Q, K)<br/>Eigenvalue Stability"]
    C --> D["ğŸ›ï¸ FineGrainedGating<br/>Î±_t = Ïƒ(Wâ†‘Wâ†“x)"]
    D --> E["ğŸ”¢ StateManager<br/>St âˆˆ R^(dkÃ—dv)"]
    E --> F["âš¡ DPLR Transition<br/>Diag(Î±) - Î²kk^T"]
    F --> G["ğŸ“¦ ChunkwiseKDA<br/>WY + UT Transform"]
    G --> H["ğŸ¯ Output Gate<br/>Ïƒ(Wâ†‘Wâ†“x) âŠ™ RMSNorm"]
    H --> I["ğŸ“¤ Output o<br/>(BÃ—TÃ—D)"]

    style A fill:#2c3e50,stroke:#3498db,stroke-width:2px,color:#ecf0f1
    style B fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style C fill:#2c3e50,stroke:#1abc9c,stroke-width:2px,color:#ecf0f1
    style D fill:#2c3e50,stroke:#e67e22,stroke-width:2px,color:#ecf0f1
    style E fill:#2c3e50,stroke:#e74c3c,stroke-width:2px,color:#ecf0f1
    style F fill:#2c3e50,stroke:#f39c12,stroke-width:2px,color:#ecf0f1
    style G fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style H fill:#2c3e50,stroke:#1abc9c,stroke-width:2px,color:#ecf0f1
    style I fill:#2c3e50,stroke:#27ae60,stroke-width:2px,color:#ecf0f1
```

### Memory State Evolution

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e1e1e','primaryTextColor':'#fff','primaryBorderColor':'#7c3aed','lineColor':'#f39c12','secondaryColor':'#2c3e50','tertiaryColor':'#1e1e1e','background':'#1e1e1e','mainBkg':'#2c3e50','secondBkg':'#34495e','textColor':'#ecf0f1','fontSize':'14px'}}}%%
stateDiagram-v2
    [*] --> S0: Initialize State
    S0 --> S1: Apply Diagonal Decay<br/>S' = Diag(Î±_t)Â·S_{t-1}
    S1 --> S2: Rank-1 Correction<br/>S'' = (I - Î²k_tk_t^T)Â·S'
    S2 --> S3: Add KV Association<br/>S_t = S'' + Î²k_tv_t^T
    S3 --> Output: Compute Output<br/>o_t = q_t^TÂ·S_t
    Output --> S3: Next Token
    S3 --> [*]: End Sequence

    note right of S0
        Constant Memory
        O(dk Ã— dv)
    end note

    note right of S2
        Delta Rule
        Online Gradient Descent
    end note
```

### Hybrid Layer Configuration

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e1e1e','primaryTextColor':'#fff','primaryBorderColor':'#7c3aed','lineColor':'#f39c12','secondaryColor':'#2c3e50','tertiaryColor':'#1e1e1e','background':'#1e1e1e','mainBkg':'#2c3e50','secondBkg':'#34495e','textColor':'#ecf0f1','fontSize':'14px'}}}%%
graph TD
    subgraph "Block 1 (3:1 Ratio)"
        A1["âš¡ KDA Layer 1"] --> A2["âš¡ KDA Layer 2"]
        A2 --> A3["âš¡ KDA Layer 3"]
        A3 --> A4["ğŸŒ MLA Layer 1"]
    end

    subgraph "Block 2 (3:1 Ratio)"
        B1["âš¡ KDA Layer 4"] --> B2["âš¡ KDA Layer 5"]
        B2 --> B3["âš¡ KDA Layer 6"]
        B3 --> B4["ğŸŒ MLA Layer 2"]
    end

    subgraph "Block N (3:1 Ratio)"
        N1["âš¡ KDA Layer N-2"] --> N2["âš¡ KDA Layer N-1"]
        N2 --> N3["âš¡ KDA Layer N"]
        N3 --> N4["ğŸŒ MLA Layer N/4"]
    end

    A4 --> B1
    B4 --> C["..."]
    C --> N1

    style A1 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style A2 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style A3 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style A4 fill:#2c3e50,stroke:#e74c3c,stroke-width:2px,color:#ecf0f1
    style B1 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style B2 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style B3 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style B4 fill:#2c3e50,stroke:#e74c3c,stroke-width:2px,color:#ecf0f1
    style N1 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style N2 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style N3 fill:#2c3e50,stroke:#9b59b6,stroke-width:2px,color:#ecf0f1
    style N4 fill:#2c3e50,stroke:#e74c3c,stroke-width:2px,color:#ecf0f1
```

### Component Details

#### Kimi Delta Attention (KDA)

```python
# Simplified KDA forward pass
def kda_forward(q, k, v, alpha, beta, state):
    """
    KDA implements:
    St = (I - Î²t kt kt^T) Diag(Î±t) St-1 + Î²t kt vt^T
    ot = qt^T St
    """
    # Step 1: Apply fine-grained diagonal decay
    state_decayed = diag(alpha) @ state  # Channel-wise forgetting

    # Step 2: Delta rule correction (Householder transform)
    correction = beta * k @ (k.T @ state_decayed)
    state_corrected = state_decayed - correction

    # Step 3: Add new key-value association
    state_new = state_corrected + beta * k @ v.T

    # Step 4: Compute output (inter-chunk + intra-chunk)
    output_inter = (q * gamma.exp()) @ state_new  # Recurrent
    output_intra = tril(q @ k.T) @ v  # Parallel

    return output_inter + output_intra, state_new
```

#### Neural Parameterization

- **Input Projections**: Q, K, V via linear layers + short convolution (kernel=4)
- **Gating**: Channel-wise forget gate (Î±), scalar learning rate (Î²)
- **Output**: Low-rank gating + RMSNorm
- **Normalization**: L2Norm for Q/K (eigenvalue stability), RMSNorm for output

---

## ï¿½ï¸ Technology Stack & Design Choices

### Core Technologies

<table>
<tr>
<th>Technology</th>
<th>Version</th>
<th>Purpose</th>
<th>Why Chosen</th>
</tr>

<tr>
<td><b>PyTorch</b></td>
<td>â‰¥2.6</td>
<td>Deep learning framework</td>
<td>
â€¢ Industry standard for research & production<br/>
â€¢ Excellent CUDA integration & autograd<br/>
â€¢ Dynamic computation graphs for debugging<br/>
â€¢ Native support for distributed training<br/>
â€¢ Extensive ecosystem (TorchScript, ONNX)
</td>
</tr>

<tr>
<td><b>CUDA</b></td>
<td>â‰¥12.0</td>
<td>GPU acceleration</td>
<td>
â€¢ Direct access to GPU hardware features<br/>
â€¢ Custom kernel optimization for KDA<br/>
â€¢ Tensor Core utilization for mixed precision<br/>
â€¢ High memory bandwidth (>900 GB/s on A100)<br/>
â€¢ Required for production-level performance
</td>
</tr>

<tr>
<td><b>Triton</b></td>
<td>â‰¥2.2</td>
<td>Kernel development</td>
<td>
â€¢ Python-based GPU kernel programming<br/>
â€¢ Automatic optimization & code generation<br/>
â€¢ Easier to maintain than raw CUDA<br/>
â€¢ Similar performance to hand-tuned CUDA<br/>
â€¢ Rapid prototyping of custom operators
</td>
</tr>

<tr>
<td><b>Flash Attention</b></td>
<td>â‰¥2.0</td>
<td>Efficient attention</td>
<td>
â€¢ Memory-efficient attention algorithm<br/>
â€¢ IO-aware kernel design (minimizes HBM access)<br/>
â€¢ Up to 3Ã— speedup over naive attention<br/>
â€¢ Industry-proven implementation<br/>
â€¢ Baseline for comparison
</td>
</tr>

<tr>
<td><b>vLLM</b></td>
<td>â‰¥0.6</td>
<td>Inference engine</td>
<td>
â€¢ PagedAttention for efficient KV cache<br/>
â€¢ Continuous batching for high throughput<br/>
â€¢ Production-grade serving infrastructure<br/>
â€¢ Easy integration with existing models<br/>
â€¢ Active community & regular updates
</td>
</tr>

<tr>
<td><b>Docker</b></td>
<td>â‰¥24.0</td>
<td>Containerization</td>
<td>
â€¢ Reproducible development environment<br/>
â€¢ Consistent CUDA/cuDNN versions<br/>
â€¢ Easy deployment to cloud platforms<br/>
â€¢ Isolation of dependencies<br/>
â€¢ Multi-stage builds for size optimization
</td>
</tr>

<tr>
<td><b>pytest</b></td>
<td>â‰¥8.0</td>
<td>Testing framework</td>
<td>
â€¢ Simple, Pythonic test syntax<br/>
â€¢ Excellent fixture system<br/>
â€¢ Parameterized testing support<br/>
â€¢ Coverage integration<br/>
â€¢ Industry standard for Python projects
</td>
</tr>

<tr>
<td><b>Black</b></td>
<td>â‰¥24.0</td>
<td>Code formatting</td>
<td>
â€¢ Opinionated, consistent formatting<br/>
â€¢ Reduces bikeshedding in reviews<br/>
â€¢ Automatic via pre-commit hooks<br/>
â€¢ Fast (written in Rust core)<br/>
â€¢ PEP 8 compliant
</td>
</tr>

<tr>
<td><b>NumPy</b></td>
<td>â‰¥1.24</td>
<td>Numerical computing</td>
<td>
â€¢ Efficient array operations<br/>
â€¢ Foundation for scientific Python<br/>
â€¢ Used for synthetic data generation<br/>
â€¢ CPU-based testing utilities<br/>
â€¢ Interoperability with PyTorch
</td>
</tr>

<tr>
<td><b>Einops</b></td>
<td>â‰¥0.8</td>
<td>Tensor manipulation</td>
<td>
â€¢ Readable tensor reshaping/rearranging<br/>
â€¢ Self-documenting dimension operations<br/>
â€¢ Reduces bugs in shape transformations<br/>
â€¢ Einstein notation support<br/>
â€¢ Clear intent for reviewers
</td>
</tr>
</table>

### Architecture Components

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e1e1e','primaryTextColor':'#fff','primaryBorderColor':'#7c3aed','lineColor':'#f39c12','secondaryColor':'#2c3e50','tertiaryColor':'#1e1e1e','background':'#1e1e1e','mainBkg':'#2c3e50','secondBkg':'#34495e','textColor':'#ecf0f1','fontSize':'12px'}}}%%
mindmap
  root((Kimi Linear<br/>Architecture))
    Core Modules
      FineGrainedGating
        Channel-wise decay
        Low-rank projection
        Sigmoid activation
      StateManager
        Fixed memory O(KÃ—V)
        Checkpointing
        NaN/Inf handling
      DPLRTransition
        Specialized DPLR
        Eigenvalue stability
        2Ã— faster vs general
    Attention Layers
      KDA Layer
        Delta rule learning
        Chunkwise parallel
        WY representation
      MLA Layer
        Global attention
        NoPE design
        Multi-head latent
    Optimization
      CUDA Kernels
        Fused operations
        Tensor Core usage
        Memory tiling
      Triton Kernels
        Auto-tuning
        Python-based
        Easy maintenance
    Memory Management
      Pre-allocated buffers
      Efficient reuse
      Secondary chunking
      Mixed precision
```

### Component Complexity Analysis

<table>
<tr>
<th>Component</th>
<th>Time Complexity</th>
<th>Space Complexity</th>
<th>Description</th>
</tr>

<tr>
<td><b>FineGrainedGating</b></td>
<td>O(BÂ·TÂ·DÂ·rank)</td>
<td>O(DÂ·rank)</td>
<td>Low-rank projection for channel-wise gates</td>
</tr>

<tr>
<td><b>StateManager</b></td>
<td>O(BÂ·HÂ·KÂ·V)</td>
<td>O(BÂ·HÂ·KÂ·V)</td>
<td>Constant per-head memory, scales with batch</td>
</tr>

<tr>
<td><b>DPLRTransition</b></td>
<td>O(BÂ·HÂ·KÂ·V)</td>
<td>O(BÂ·HÂ·KÂ·V)</td>
<td>2Ã— faster than general DPLR (O(KÂ²Â·V))</td>
</tr>

<tr>
<td><b>ChunkwiseKDA</b></td>
<td>O(BÂ·TÂ·KÂ·V + TÂ·CÂ²)</td>
<td>O(BÂ·HÂ·KÂ·V)</td>
<td>Parallel intra-chunk + recurrent inter-chunk</td>
</tr>

<tr>
<td><b>Full MLA</b></td>
<td>O(BÂ·TÂ²Â·D)</td>
<td>O(BÂ·HÂ·TÂ·K)</td>
<td>Standard attention with linear KV cache growth</td>
</tr>

<tr>
<td><b>Hybrid Model</b></td>
<td>O(BÂ·TÂ·DÂ·V + TÂ²Â·D/4)</td>
<td>O(BÂ·HÂ·KÂ·V + TÂ·D/4)</td>
<td>3:1 ratio reduces global attention cost by 75%</td>
</tr>
</table>

### Key Design Decisions

| Decision | Rationale | Trade-offs |
|----------|-----------|------------|
| **Channel-wise vs Head-wise Gating** | More precise memory control, better long-context performance | Slightly higher parameter count (~1%) |
| **3:1 KDA-to-MLA Ratio** | Optimal balance of speed and accuracy | Tunable for specific use cases |
| **NoPE (No Position Encoding)** | Simplifies long-context extension, KDA provides positional bias | Requires careful training schedule |
| **Pre-allocated State Buffer** | Eliminates allocation overhead, predictable memory | Fixed maximum batch size |
| **WY Representation** | Efficient Householder matrix products | More complex implementation |
| **Secondary Chunking** | Numerical stability in log-space | Additional memory overhead |
| **Eigenvalue Monitoring** | Early detection of training instabilities | Small runtime cost (<1%) |
| **Low-rank Gate Projection** | Reduces parameters while maintaining expressiveness | Slightly lower capacity |

---

##  Performance

### Scaling Visualization

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e1e1e','mainBkg':'#2c3e50','secondBkg':'#34495e','textColor':'#ecf0f1','fontSize':'14px','primaryTextColor':'#ecf0f1','primaryBorderColor':'#3498db'}}}%%
graph TD
    A[Context Length: 4K] -->|"MLA: 2.1ms<br/>Kimi: 2.0ms"| B[Speed: 1.05Ã— faster]
    C[Context Length: 128K] -->|"MLA: 45.2ms<br/>Kimi: 11.4ms"| D[Speed: 3.98Ã— faster âš¡]
    E[Context Length: 512K] -->|"MLA: 182.7ms<br/>Kimi: 79.4ms"| F[Speed: 2.30Ã— faster]
    G[Context Length: 1M] -->|"MLA: 365.4ms<br/>Kimi: 125.8ms"| H[Speed: 2.90Ã— faster]

    I[Memory @ 128K] -->|"MLA: 16GB<br/>Kimi: 4GB"| J[75% reduction ğŸ’¾]
    K[Memory @ 1M] -->|"MLA: 128GB<br/>Kimi: 32GB"| L[75% reduction ğŸ’¾]

    style A fill:#2c3e50,stroke:#3498db,stroke-width:2px
    style C fill:#2c3e50,stroke:#3498db,stroke-width:2px
    style E fill:#2c3e50,stroke:#3498db,stroke-width:2px
    style G fill:#2c3e50,stroke:#3498db,stroke-width:2px
    style I fill:#2c3e50,stroke:#9b59b6,stroke-width:2px
    style K fill:#2c3e50,stroke:#9b59b6,stroke-width:2px
    style B fill:#34495e,stroke:#27ae60,stroke-width:2px
    style D fill:#34495e,stroke:#27ae60,stroke-width:3px
    style F fill:#34495e,stroke:#27ae60,stroke-width:2px
    style H fill:#34495e,stroke:#27ae60,stroke-width:2px
    style J fill:#34495e,stroke:#e74c3c,stroke-width:2px
    style L fill:#34495e,stroke:#e74c3c,stroke-width:2px
```

### Speed Benchmarks (Prefill Stage)

<table>
<tr>
<th>Context Length</th>
<th>MLA (ms)</th>
<th>GDN-H (ms)</th>
<th>Kimi Linear (ms)</th>
<th>Speedup vs MLA</th>
<th>Winner</th>
</tr>

<tr>
<td><b>4K</b></td>
<td>2.1</td>
<td>2.0</td>
<td>2.0</td>
<td>1.05Ã—</td>
<td>ğŸŸ° Tie</td>
</tr>

<tr>
<td><b>128K</b></td>
<td>45.2</td>
<td>18.3</td>
<td>11.4</td>
<td><b>3.98Ã—</b></td>
<td>âš¡ Kimi</td>
</tr>

<tr>
<td><b>512K</b></td>
<td>182.7</td>
<td>76.1</td>
<td>79.4</td>
<td><b>2.30Ã—</b></td>
<td>âš¡ Kimi</td>
</tr>

<tr>
<td><b>1M</b></td>
<td>365.4</td>
<td>150.2</td>
<td>125.8</td>
<td><b>2.90Ã—</b></td>
<td>âš¡ Kimi</td>
</tr>
</table>

### Decoding TPOT (Time Per Output Token)

| Context Length | MLA TPOT | Kimi TPOT | Speedup | Insight |
|----------------|----------|-----------|---------|---------|
| **4K** | 1.85 ms | 1.84 ms | **1.01Ã—** | Minimal difference at short context |
| **128K** | 4.28 ms | 1.91 ms | **2.24Ã—** âš¡ | Linear KV cache starts to dominate |
| **512K** | 9.16 ms | 1.87 ms | **4.90Ã—** âš¡âš¡ | Massive savings from O(1) state |
| **1M** | 11.48 ms | 1.84 ms | **6.24Ã—** âš¡âš¡âš¡ | 6Ã— faster decoding! |

> **Key Insight**: Kimi Linear maintains **constant TPOT** (~1.84ms) regardless of context length, while MLA's TPOT grows linearly. This enables sub-2ms per-token generation even at 1M context!

### Memory Efficiency Comparison

<table>
<tr>
<th>Metric</th>
<th>Full Attention (MLA)</th>
<th>Kimi Linear</th>
<th>Reduction</th>
<th>Impact</th>
</tr>

<tr>
<td><b>KV Cache @ 4K</b></td>
<td>512 MB</td>
<td>512 MB</td>
<td>0%</td>
<td>No advantage at short context</td>
</tr>

<tr>
<td><b>KV Cache @ 128K</b></td>
<td>16.0 GB</td>
<td>4.0 GB</td>
<td><b>75%</b> ğŸ’¾</td>
<td>4Ã— larger batch size possible</td>
</tr>

<tr>
<td><b>Peak Memory @ 512K</b></td>
<td>64.0 GB</td>
<td>16.0 GB</td>
<td><b>75%</b> ğŸ’¾</td>
<td>Fits on single A100 40GB</td>
</tr>

<tr>
<td><b>Peak Memory @ 1M</b></td>
<td>128.0 GB</td>
<td>32.0 GB</td>
<td><b>75%</b> ğŸ’¾</td>
<td>Practical million-token inference</td>
</tr>

<tr>
<td><b>State Growth</b></td>
<td>O(n) per head</td>
<td>O(1) per head</td>
<td>N/A</td>
<td>Bounded memory even at âˆ context</td>
</tr>

<tr>
<td><b>Batch Throughput</b></td>
<td>Limited by KV cache</td>
<td>4Ã— higher @ 128K</td>
<td><b>4Ã—</b> ğŸ“Š</td>
<td>Better hardware utilization</td>
</tr>
</table>

### Attention Mechanism Comparison

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e1e1e','mainBkg':'#2c3e50','secondBkg':'#34495e','textColor':'#ecf0f1','fontSize':'13px'}}}%%
graph LR
    A[Full Attention<br/>O nÂ² complexity] -->|"Short Context<br/>< 4K"| B[âœ… Best Accuracy<br/>âŒ Slow scaling]
    C[Linear Attention<br/>O n complexity] -->|"Medium Context<br/>4K-128K"| D[âœ… Fast<br/>âŒ Accuracy loss]
    E[Kimi Hybrid<br/>O n with sparse O nÂ²] -->|"Long Context<br/>128K-1M"| F[âœ… Fast + Accurate<br/>âœ… Constant memory]

    style A fill:#2c3e50,stroke:#e74c3c,stroke-width:2px
    style C fill:#2c3e50,stroke:#f39c12,stroke-width:2px
    style E fill:#2c3e50,stroke:#27ae60,stroke-width:3px
    style B fill:#34495e,stroke:#3498db,stroke-width:2px
    style D fill:#34495e,stroke:#3498db,stroke-width:2px
    style F fill:#34495e,stroke:#27ae60,stroke-width:3px
```

### Accuracy Benchmarks

<table>
<tr>
<th>Task</th>
<th>Context</th>
<th>MLA (Full Attn)</th>
<th>GDN-H (Linear)</th>
<th>Kimi Linear</th>
<th>Winner</th>
</tr>

<tr>
<td><b>MMLU-Pro</b></td>
<td>4K</td>
<td>47.2</td>
<td>47.9</td>
<td><b>51.0</b></td>
<td>âœ… Kimi (+3.8)</td>
</tr>

<tr>
<td><b>RULER</b></td>
<td>128K</td>
<td>81.3</td>
<td>80.5</td>
<td><b>84.3</b></td>
<td>âœ… Kimi (+3.0)</td>
</tr>

<tr>
<td><b>MATH500</b></td>
<td>4K</td>
<td>80.8</td>
<td><b>83.0</b></td>
<td>81.2</td>
<td>ğŸ¥ˆ Kimi (+0.4)</td>
</tr>

<tr>
<td><b>AIME 2025</b></td>
<td>4K</td>
<td>20.6</td>
<td>21.1</td>
<td><b>21.3</b></td>
<td>âœ… Kimi (+0.7)</td>
</tr>

<tr>
<td><b>HumanEval</b></td>
<td>4K</td>
<td>71.3</td>
<td>72.0</td>
<td><b>73.2</b></td>
<td>âœ… Kimi (+1.9)</td>
</tr>

<tr>
<td><b>GPQA</b></td>
<td>4K</td>
<td><b>44.2</b></td>
<td>43.1</td>
<td>43.8</td>
<td>ğŸ¥ˆ Kimi (-0.4)</td>
</tr>
</table>

**Summary**: Kimi Linear achieves **better or comparable accuracy** to full attention while being **2-6Ã— faster** at long context. The hybrid approach avoids the accuracy degradation typical of pure linear attention.

### Throughput Scaling

| Batch Size | Context | MLA Tokens/sec | Kimi Tokens/sec | Throughput Gain |
|------------|---------|----------------|-----------------|-----------------|
| 1 | 128K | 234 | 524 | **2.24Ã—** âš¡ |
| 4 | 128K | 890 | 1987 | **2.23Ã—** âš¡ |
| 8 | 128K | OOM | 3840 | **âˆ** ğŸ’¥ |
| 1 | 1M | 87 | 543 | **6.24Ã—** âš¡âš¡âš¡ |
| 4 | 1M | OOM | 2048 | **âˆ** ğŸ’¥ |

**Hardware**: A100 80GB, BF16, DeepSpeed ZeRO-3

> **Key Takeaway**: At 1M context, Kimi Linear enables 4Ã— batch size that causes OOM in MLA, unlocking previously impossible workloads.

---

## ğŸš€ Installation

### Prerequisites

- Python >= 3.10
- PyTorch >= 2.6
- CUDA >= 12.0 (for GPU acceleration)
- fla-core >= 0.4.0

### Option 1: From Source (Recommended for Development)

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/kimi-linear.git
cd kimi-linear

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .

**Decoding TPOT (Time Per Output Token)**:
- 4K: 1.84ms (Kimi Linear) vs 1.85ms (MLA) = 1.01Ã— speedup
- 1M: 1.84ms (Kimi Linear) vs 11.48ms (MLA) = **6.3Ã— speedup** âš¡

### Memory Efficiency

| Metric | Full Attention (MLA) | Kimi Linear | Reduction |
|--------|----------------------|-------------|-----------|
| KV Cache @ 128K | 16.0 GB | 4.0 GB | **75%** |
| Peak Memory @ 1M | 128.0 GB | 32.0 GB | **75%** |
| State Size per Head | Linear (O(n)) | Constant (dk Ã— dv) | N/A |

### Accuracy Benchmarks

| Task | Context | MLA | GDN-H | Kimi Linear |
|------|---------|-----|-------|-------------|
| MMLU-Pro | 4K | 47.2 | 47.9 | **51.0** âœ… |
| RULER | 128K | 81.3 | 80.5 | **84.3** âœ… |
| MATH500 | 4K | 80.8 | 83.0 | **81.2** |
| AIME 2025 | 4K | 20.6 | 21.1 | **21.3** âœ… |

---

## ğŸš€ Installation

### Prerequisites

- Python >= 3.10
- PyTorch >= 2.6
- CUDA >= 12.0 (for GPU acceleration)
- fla-core >= 0.4.0

### Option 1: From Source (Recommended for Development)

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/kimi-linear.git
cd kimi-linear

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

### Option 2: Using Docker

```bash
# Build the Docker image
docker build -t kimi-linear:latest -f docker/Dockerfile .

# Run the container
docker run --gpus all -it kimi-linear:latest
```

### Option 3: Using pip (Future)

```bash
# Once published to PyPI
pip install kimi-linear
```

---

## ğŸ¯ Quick Start

### Basic Usage

```python
import torch
from kimi_linear import KimiLinearAttention

# Initialize model
model = KimiLinearAttention(
    dim=1024,
    num_heads=16,
    head_dim=128,
    hybrid_ratio=3,  # 3 KDA layers per 1 MLA layer
)

# Forward pass
x = torch.randn(1, 4096, 1024)  # (batch, seq_len, dim)
output = model(x)

print(f"Output shape: {output.shape}")  # (1, 4096, 1024)
```

### Inference with Pre-trained Model

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "moonshotai/Kimi-Linear-48B-A3B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Explain Kimi Linear in simple terms."}
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

generated_ids = model.generate(inputs=input_ids, max_new_tokens=500)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)
```

### Running Benchmarks

```bash
# Run all benchmarks
python scripts/benchmark/run_benchmarks.py --model kimi-linear --baseline mla

# Run specific benchmark
python scripts/benchmark/run_benchmarks.py --task mmlu-pro --context-length 4096

# Profile performance
python scripts/profiling/profile_attention.py --kernel kda --chunk-size 64
```

### Running Tests

```bash
# Run all tests
pytest tests/

# Run unit tests only
pytest tests/unit/

# Run with coverage
pytest --cov=src --cov-report=html tests/

# Run synthetic tasks
python tests/synthetic/test_palindrome.py
python tests/synthetic/test_mqar.py
python tests/synthetic/test_stack.py
```

---

## ğŸ“‚ Project Structure

```
kimi-linear/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ kda/                      # Kimi Delta Attention implementation
â”‚   â”‚   â”œâ”€â”€ gating.py            # Fine-grained gating mechanism
â”‚   â”‚   â”œâ”€â”€ state_manager.py    # State tracking and updates
â”‚   â”‚   â”œâ”€â”€ wy_representation.py # WY representation for rank-1 updates
â”‚   â”‚   â”œâ”€â”€ ut_transform.py      # UT transform
â”‚   â”‚   â”œâ”€â”€ chunk_update.py      # Chunkwise state updates
â”‚   â”‚   â””â”€â”€ dplr.py              # DPLR variant implementation
â”‚   â”œâ”€â”€ attention/                # Attention mechanisms
â”‚   â”‚   â”œâ”€â”€ linear_attention.py  # Base linear attention
â”‚   â”‚   â”œâ”€â”€ delta_rule.py        # Delta rule learning
â”‚   â”‚   â””â”€â”€ mla.py               # Multi-Head Latent Attention
â”‚   â”œâ”€â”€ models/                   # Model architectures
â”‚   â”‚   â”œâ”€â”€ kimi_linear.py       # Hybrid Kimi Linear model
â”‚   â”‚   â”œâ”€â”€ projections.py       # Input projections
â”‚   â”‚   â”œâ”€â”€ conv_layer.py        # Short convolution
â”‚   â”‚   â””â”€â”€ gating.py            # Output/forget gates
â”‚   â”œâ”€â”€ kernels/                  # Optimized kernels
â”‚   â”‚   â”œâ”€â”€ kda_fused_kernel.cu  # CUDA implementation
â”‚   â”‚   â””â”€â”€ kda_triton.py        # Triton implementation
â”‚   â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”‚   â”œâ”€â”€ performance_logger.py
â”‚   â”‚   â””â”€â”€ memory_monitor.py
â”‚   â””â”€â”€ benchmarks/               # Benchmark utilities
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ unit/                    # Unit tests
â”‚   â”œâ”€â”€ integration/             # Integration tests
â”‚   â””â”€â”€ synthetic/               # Synthetic task tests
â”œâ”€â”€ scripts/                      # Scripts
â”‚   â”œâ”€â”€ setup/                   # Setup scripts
â”‚   â”œâ”€â”€ benchmark/               # Benchmarking scripts
â”‚   â””â”€â”€ profiling/               # Profiling tools
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ api/                     # API documentation
â”‚   â”œâ”€â”€ tutorials/               # Tutorials and guides
â”‚   â”œâ”€â”€ architecture/            # Architecture docs
â”‚   â””â”€â”€ project-plan.md          # Comprehensive project plan
â”œâ”€â”€ data/                         # Data directory
â”‚   â”œâ”€â”€ synthetic/               # Synthetic test data
â”‚   â”œâ”€â”€ benchmarks/              # Benchmark results
â”‚   â””â”€â”€ results/                 # Experimental results
â”œâ”€â”€ assets/                       # Assets (figures, diagrams)
â”œâ”€â”€ docker/                       # Docker configurations
â”œâ”€â”€ .github/                      # GitHub-specific files
â”‚   â””â”€â”€ workflows/               # CI/CD workflows
â”œâ”€â”€ .copilot/                     # Copilot configurations
â”œâ”€â”€ .vscode/                      # VS Code settings
â”œâ”€â”€ memory-bank/                  # Memory bank system
â”‚   â”œâ”€â”€ app-description.md       # Project description
â”‚   â”œâ”€â”€ change-log.md            # Change log
â”‚   â””â”€â”€ implementation-plans/    # Implementation plans
â”œâ”€â”€ configs/                      # Configuration files
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ setup.py                      # Package setup
â”œâ”€â”€ pyproject.toml               # Project metadata
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ .editorconfig                # Editor configuration
â”œâ”€â”€ LICENSE                      # MIT License
â””â”€â”€ README.md                    # This file
```

---

## ğŸ› ï¸ Development

### Setting Up Development Environment

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run code formatting
black src/ tests/
isort src/ tests/

# Run linting
pylint src/
flake8 src/

# Run type checking
mypy src/
```

### Building Documentation

```bash
cd docs/
make html
# Documentation will be in docs/_build/html/
```

### Running in Docker (Development)

```bash
# Build development image
docker build -t kimi-linear:dev -f docker/Dockerfile.dev .

# Run with GPU and mounted source
docker run --gpus all -v $(pwd):/workspace -it kimi-linear:dev bash
```

### Code Style Guidelines

- **Python**: Follow PEP 8, use Black formatter (88 char line length)
- **C++**: Follow Google C++ Style Guide (100 char line length)
- **Java**: Follow Google Java Style Guide
- **Naming Conventions**:
  - Functions/methods: `snake_case`
  - Classes: `PascalCase`
  - Constants: `UPPER_SNAKE_CASE`
  - Private members: `_leading_underscore`

---

## ğŸ“Š Benchmarks

### Running Comprehensive Benchmarks

```bash
# Full benchmark suite (requires GPU with 24GB+ VRAM)
python scripts/benchmark/run_benchmarks.py \
    --models kimi-linear mla gdn-h \
    --context-lengths 4096 32768 131072 524288 1048576 \
    --tasks all \
    --output-dir data/benchmarks/results

# Quick benchmark (lighter tests)
python scripts/benchmark/run_benchmarks.py \
    --models kimi-linear mla \
    --context-lengths 4096 32768 \
    --tasks mmlu-pro ruler \
    --quick
```

### Synthetic Task Evaluation

```bash
# Palindrome test (sequence reversal)
python tests/synthetic/test_palindrome.py --lengths 256 512 1024 2048

# MQAR test (associative recall)
python tests/synthetic/test_mqar.py --num-queries 5 10 20

# Stack test (state tracking)
python tests/synthetic/test_stack.py --num-stacks 64 --sequence-length 1024
```

### Performance Profiling

```bash
# Kernel profiling with Nsight Compute
ncu --set full python scripts/profiling/profile_kernels.py

# System profiling with Nsight Systems
nsys profile -o profile.nsys-rep python scripts/profiling/profile_system.py

# Memory profiling
python scripts/profiling/profile_memory.py --max-context 1048576
```

---

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` directory:

- **[Quick Start Guide](docs/tutorials/quickstart.md)**: Get started in 5 minutes
- **[API Reference](docs/api/)**: Complete API documentation
- **[Architecture Guide](docs/architecture/)**: Deep dive into the architecture
- **[Training Guide](docs/tutorials/training.md)**: Training your own models
- **[Advanced Usage](docs/tutorials/advanced.md)**: Custom kernels and optimizations
- **[Project Plan](docs/project-plan.md)**: Comprehensive development roadmap

### Additional Resources

- **Research Paper**: [Kimi Linear Technical Report](https://arxiv.org/abs/2510.26692)
- **Original Implementation**: [MoonshotAI/Kimi-Linear](https://github.com/MoonshotAI/Kimi-Linear)
- **FLA Kernels**: [fla-org/flash-linear-attention](https://github.com/fla-org/flash-linear-attention)
- **Pre-trained Models**: [HuggingFace Hub](https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct)

---

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](.github/CONTRIBUTING.md) for details.

### How to Contribute

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Make your changes**
4. **Run tests** (`pytest tests/`)
5. **Commit your changes** (`git commit -m 'Add amazing feature'`)
6. **Push to branch** (`git push origin feature/amazing-feature`)
7. **Open a Pull Request**

### Development Priorities

- ğŸ”´ **Critical**: Core functionality, bug fixes, performance regressions
- ğŸŸ  **High**: New features, optimizations
- ğŸŸ¡ **Medium**: Documentation improvements, refactoring
- ğŸŸ¢ **Low**: Code style, minor enhancements

---

## ï¿½ï¿½ Citation

If you use Kimi Linear in your research, please cite:

```bibtex
@misc{team2025kimi,
  title         = {Kimi Linear: An Expressive, Efficient Attention Architecture},
  author        = {Zhang, Yu and Lin, Zongyu and Yao, Xingcheng and Hu, Jiaxi and others},
  year          = {2025},
  eprint        = {2510.26692},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Moonshot AI** for the original Kimi Linear research and implementation
- **FLA Team** for the flash linear attention kernels
- **DeepSeek** for MLA architecture insights
- **Community contributors** for feedback and improvements

---

## ğŸ“¬ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/YOUR_USERNAME/kimi-linear/issues)
- **Discussions**: [GitHub Discussions](https://github.com/YOUR_USERNAME/kimi-linear/discussions)

---

<div align="center">

**â­ Star this repository if you find it useful!**

Made with â¤ï¸ by the Kimi Linear Optimization Team

</div>
